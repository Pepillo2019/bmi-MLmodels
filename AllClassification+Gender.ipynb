{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "##1st script /Users/jose/Documents/Python/py_scripts/ClassificationModels.py (select best model)\nimport pandas as pd\n#models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC \nfrom sklearn.neural_network import MLPClassifier #NN\n#Emsemble model\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier, GradientBoostingClassifier\n\nimport warnings\n# To ignore all warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load the CSV file as a Pandas DataFrame\n#data = pd.read_csv('Gender_Height_Weight.csv') #training set\ndata = pd.read_csv('Health_And_Nutrition_BmiClass.Training.csv') #training set\n#r = pd.read_csv('Gender.values.csv') #testing set\nr = pd.read_csv('Health_And_Nutrition_BmiClass.Test.csv') #testing set\n\n# Divide the DataFrame into features and labels\nX = data[['Gender', 'Height', 'Weight']]\ny = data['BMI_Code']\n\n# Create a list of models\nmodels = [\n    ('decision_tree', DecisionTreeClassifier()),\n    ('k_neighbors', KNeighborsClassifier(n_neighbors=5)),\n    ('svr', SVC()),\n    ('mlp_classifier', MLPClassifier()),#NN\n    ('linear_regression', LogisticRegression()), ('ridge_regression', RidgeClassifier())\n]\n\n# Create an ensemble model with the models\n#model = VotingClassifier(models)\n# Train the model with the data\n#model.fit(X, y)\n\n# Train individual models\nfor name, model in models:\n    model.fit(X, y)\n\n# Create ensemble models\nvc_model = VotingClassifier([(name, model) for name, model in models])\nrf_model = RandomForestClassifier()\ngb_model = GradientBoostingClassifier()\n\n# Train ensemble models\nvc_model.fit(X, y)\nrf_model.fit(X, y)\ngb_model.fit(X, y)\n\n# Open the output CSV file for writing\nwith open('outputClassificationModelsGender.csv', 'w') as w:\n    # Write the header to the CSV file\n    w.write('Index1,Index2,Gender_Code,Height,Weight,Bmi_Code,DecisionTree,KNeighbors,SVR,NeuralNetwork,LogisticRegression,RidgeClassifier,Voting,RandomForest,GradientBoosting\\n')\n\n    # Loop through rows in the values.csv file\n    for index, row in r.iterrows():\n        col = row[['Index', 'Gender', 'Height', 'Weight', 'BMI_Code']]\n        Index, gender, height, weight, bmiC = col  # Unpack values from the row\n\n    # Make predictions with the models\n        #Bmi_predicha = model.predict([[gender, height, weight]])\n        #Bmi1 = model.estimators_[0].predict([[gender, height, weight]])\n        #Bmi2 = model.estimators_[1].predict([[gender, height, weight]])\n        #Bmi3 = model.estimators_[2].predict([[gender, height, weight]])\n        #Bmi4 = model.estimators_[3].predict([[gender, height, weight]])\n        #Bmi5 = model.estimators_[4].predict([[gender, height, weight]])\n        #Bmi6 = model.estimators_[5].predict([[gender, height, weight]])\n        Bmi1 = models[0][1].predict([[gender, height, weight]])\n        Bmi2 = models[1][1].predict([[gender, height, weight]])\n        Bmi3 = models[2][1].predict([[gender, height, weight]])\n        Bmi4 = models[3][1].predict([[gender, height, weight]])\n        Bmi5 = models[4][1].predict([[gender, height, weight]])\n        Bmi6 = models[5][1].predict([[gender, height, weight]])\n    \n        #ensemblers predictions\n        Bmi_predicha_vc = vc_model.predict([[gender, height, weight]])\n        Bmi_predicha_rf = rf_model.predict([[gender, height, weight]])\n        Bmi_predicha_gb = gb_model.predict([[gender, height, weight]])\n\n        # Write the results to the CSV file\n        w.write(f'{index},{Index},{gender},{height},{weight},{bmiC},{Bmi1[0]},{Bmi3[0]},{Bmi4[0]},{Bmi5[0]},{Bmi2[0]},{Bmi6[0]},{Bmi_predicha_vc[0]},{Bmi_predicha_rf[0]},{Bmi_predicha_gb[0]}\\n')\n\n        # Print the results\n        print(f'{index},{Index},{gender},{bmiC},DecisionTree {Bmi1[0]},'\n              f'KNeighbors {Bmi3[0]},SupportVector {Bmi4[0]},NeuralNetwork {Bmi5[0]},'\n              f'LinearRegression {Bmi2[0]},RidgeRegression {Bmi6[0]},Voting {Bmi_predicha_vc[0]}, RandomForest {Bmi_predicha_rf[0]}, GradientBoosting {Bmi_predicha_gb[0]}')",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "0,0,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n1,1,1,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n2,2,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n3,3,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n4,4,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n5,5,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n6,6,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n7,7,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n8,8,1,1,DecisionTree 1,KNeighbors 1,SupportVector 0,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n9,9,1,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n10,10,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n11,11,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n12,12,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n13,13,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n14,14,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 2,LinearRegression 1,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n15,15,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n16,16,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n17,17,2,1,DecisionTree 1,KNeighbors 1,SupportVector 0,NeuralNetwork 0,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n18,18,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n19,19,2,1,DecisionTree 1,KNeighbors 1,SupportVector 0,NeuralNetwork 0,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n20,20,1,1,DecisionTree 0,KNeighbors 1,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n21,21,2,2,DecisionTree 2,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 2,RidgeRegression 2,Voting 1, RandomForest 2, GradientBoosting 2\n22,22,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n23,23,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n24,24,1,1,DecisionTree 0,KNeighbors 1,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 1,Voting 0, RandomForest 1, GradientBoosting 0\n25,25,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n26,26,1,1,DecisionTree 1,KNeighbors 1,SupportVector 0,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n27,27,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n28,28,1,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n29,29,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n30,30,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n31,31,1,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n32,32,1,0,DecisionTree 0,KNeighbors 0,SupportVector 1,NeuralNetwork 1,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n33,33,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n34,34,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n35,35,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n36,36,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 1,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n37,37,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n38,38,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n39,39,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n40,40,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n41,41,1,0,DecisionTree -1,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression -1,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting -1\n42,42,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n43,43,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n44,44,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n45,45,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n46,46,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n47,47,2,1,DecisionTree 2,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 2,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n48,48,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n49,49,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n50,50,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n51,51,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n52,52,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n53,53,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n54,54,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n55,55,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n56,56,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 1,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n57,57,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n58,58,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n59,59,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n60,60,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n61,61,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n62,62,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n63,63,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n64,64,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n65,65,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n66,66,1,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n67,67,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 1\n68,68,1,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n69,69,2,-1,DecisionTree -1,KNeighbors 0,SupportVector -1,NeuralNetwork -1,LinearRegression -1,RidgeRegression 0,Voting -1, RandomForest -1, GradientBoosting -1\n70,70,1,2,DecisionTree 2,KNeighbors 2,SupportVector 1,NeuralNetwork 1,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n71,71,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n72,72,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n73,73,2,1,DecisionTree 1,KNeighbors 1,SupportVector 0,NeuralNetwork 0,LinearRegression 1,RidgeRegression 2,Voting 1, RandomForest 1, GradientBoosting 1\n74,74,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n75,75,1,2,DecisionTree 2,KNeighbors 2,SupportVector 1,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n76,76,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n77,77,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n78,78,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n79,79,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n80,80,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n81,81,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n82,82,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n83,83,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 1,Voting 1, RandomForest 1, GradientBoosting 1\n84,84,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n85,85,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 1,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n86,86,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n87,87,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n88,88,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n89,89,2,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n90,90,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n91,91,1,1,DecisionTree 0,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 1\n92,92,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n93,93,2,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n94,94,1,1,DecisionTree 1,KNeighbors 1,SupportVector 1,NeuralNetwork 1,LinearRegression 1,RidgeRegression 0,Voting 1, RandomForest 1, GradientBoosting 1\n95,95,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n96,96,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n97,97,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n98,98,2,2,DecisionTree 2,KNeighbors 2,SupportVector 2,NeuralNetwork 2,LinearRegression 2,RidgeRegression 2,Voting 2, RandomForest 2, GradientBoosting 2\n99,99,1,0,DecisionTree 0,KNeighbors 0,SupportVector 0,NeuralNetwork 0,LinearRegression 0,RidgeRegression 0,Voting 0, RandomForest 0, GradientBoosting 0\n100,113,2,-1,DecisionTree -1,KNeighbors 0,SupportVector -1,NeuralNetwork -1,LinearRegression -1,RidgeRegression 0,Voting -1, RandomForest -1, GradientBoosting -1\n101,250,1,-1,DecisionTree -1,KNeighbors 0,SupportVector -1,NeuralNetwork -1,LinearRegression -1,RidgeRegression 0,Voting -1, RandomForest -1, GradientBoosting -1\n102,368,1,-1,DecisionTree -1,KNeighbors 0,SupportVector -1,NeuralNetwork 0,LinearRegression -1,RidgeRegression 0,Voting 0, RandomForest -1, GradientBoosting -1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "##2nd script /Users/jose/Documents/Python/py_scripts/GraphStats.py (produce graphs and stats: MAE,MSE,RMSE,R-squared)\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Read data from input file\ninput_file = 'outputClassificationModelsGender.csv'  # Replace with your actual file path\ndf = pd.read_csv(input_file)  # Assuming tab-separated data\n\n# Assuming df is your DataFrame with model difference columns\n#for best models and ensemblers\ncolumns_to_compare = ['DecisionTree', 'KNeighbors', 'SVR', 'NeuralNetwork', 'LogisticRegression', 'RidgeClassifier', 'Voting', 'RandomForest', 'GradientBoosting']\n\n# Statistics\nmae_values = {model: mean_absolute_error(df['Bmi_Code'], df[model]) for model in columns_to_compare}\nmse_values = {model: mean_squared_error(df['Bmi_Code'], df[model]) for model in columns_to_compare}\nrmse_values = {model: mean_squared_error(df['Bmi_Code'], df[model], squared=False) for model in columns_to_compare}\nr2_values = {model: r2_score(df['Bmi_Code'], df[model]) for model in columns_to_compare}\n\n# Print and write statistics to output file\noutput_file = 'StatsClassificationModelsGender.csv'  # Replace with your desired output file path\nwith open(output_file, 'w') as w:\n    w.write('Model,MAE,MSE,RMSE,R-squared\\n')\n    for model in columns_to_compare:\n        w.write(f'{model},{mae_values[model]:.4f},{mse_values[model]:.4f},{rmse_values[model]:.4f},{r2_values[model]:.4f}\\n')\n\n# Display statistics\n    print('Model\\t\\tMAE\\t\\tMSE\\t\\tRMSE\\t\\tR-squared')\n    for model in columns_to_compare:\n        print(f'{model}\\t{mae_values[model]:.4f}\\t{mse_values[model]:.4f}\\t{rmse_values[model]:.4f}\\t{r2_values[model]:.4f}')",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model\t\tMAE\t\tMSE\t\tRMSE\t\tR-squared\nDecisionTree\t0.0485\t0.0485\t0.2203\t0.9326\nKNeighbors\t0.0485\t0.0485\t0.2203\t0.9326\nSVR\t0.1068\t0.1068\t0.3268\t0.8518\nNeuralNetwork\t0.1262\t0.1262\t0.3553\t0.8248\nLogisticRegression\t0.0485\t0.0485\t0.2203\t0.9326\nRidgeClassifier\t0.2913\t0.2913\t0.5397\t0.5958\nVoting\t0.0485\t0.0485\t0.2203\t0.9326\nRandomForest\t0.0194\t0.0194\t0.1393\t0.9731\nGradientBoosting\t0.0388\t0.0388\t0.1971\t0.9461\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "##3rd script /Users/jose/Documents/Python/py_scripts/APRF1metrics.py (produce graphs and stats: Accuracy,Precision,Recall,F1-score)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# Initialize empty lists to store metrics\naccuracy_list = []\nprecision_list = []\nrecall_list = []\nf1_list = []\n\n# Evaluate classification metrics for each model\nfor model in columns_to_compare:\n    # Choose an appropriate threshold based on your data characteristics\n    threshold = 0.5  # Adjust this threshold, e.g., 0.0 for any positive difference\n\n    y_true = (df['Bmi_Code'] >= 25).astype(int)\n    y_pred = (df[model] >= threshold).astype(int)\n\n    # Calculate classification metrics\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n\n    # Append metrics to lists\n    accuracy_list.append(accuracy)\n    precision_list.append(precision)\n    recall_list.append(recall)\n    f1_list.append(f1)\n\n# Print and write metrics to output file\noutput_file = 'StatsClassificationModelsGender2.csv'  # Replace with your desired output file path\nwith open(output_file, 'w') as w:\n    w.write('Model,Accuracy,Precision,Recall,F1-score\\n')\n    for i, model in enumerate(columns_to_compare):\n        w.write(f'{model},{accuracy_list[i]:.4f},{precision_list[i]:.4f},{recall_list[i]:.4f},{f1_list[i]:.4f}\\n')\n\n# Display metrics\nprint('Model\\tAccuracy\\tPrecision\\tRecall\\tF1-score')\nfor i, model in enumerate(columns_to_compare):\n    print(f'{model}\\t{accuracy_list[i]:.4f}\\t{precision_list[i]:.4f}\\t{recall_list[i]:.4f}\\t{f1_list[i]:.4f}')",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Model\tAccuracy\tPrecision\tRecall\tF1-score\nDecisionTree\t0.3592\t1.0000\t0.3592\t0.5286\nKNeighbors\t0.3301\t1.0000\t0.3301\t0.4964\nSVR\t0.3883\t1.0000\t0.3883\t0.5594\nNeuralNetwork\t0.3398\t1.0000\t0.3398\t0.5072\nLogisticRegression\t0.3592\t1.0000\t0.3592\t0.5286\nRidgeClassifier\t0.4951\t1.0000\t0.4951\t0.6623\nVoting\t0.3592\t1.0000\t0.3592\t0.5286\nRandomForest\t0.3495\t1.0000\t0.3495\t0.5180\nGradientBoosting\t0.3398\t1.0000\t0.3398\t0.5072\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": "##4th script /Users/jose/Documents/Python/py_scripts/Classification_report.py (extra stats: Accuracy,Precision,Recall,F1-score)\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n#from sklearn.metrics import accuracy_score\n#import seaborn as sns\n\n# Print and write metrics to output file\noutput_file = 'StatsClassificationModelsGender3.csv'  # Replace with your desired output file path\nwith open(output_file, 'w') as w:\n        \n# Evaluate classification metrics for each model\n    for model in columns_to_compare:\n    # Choose an appropriate threshold based on your data characteristics\n        threshold = 0.5  # Adjust this threshold, e.g., 0.0 for any positive difference\n\n        y_true = df['Bmi_Code']\n        y_pred = (df[model] >= threshold).astype(int)\n\n    # Print confusion matrix and classification report\n        classification_result = classification_report(y_true, y_pred)\n        accuracy = accuracy_score(y_true, y_pred)\n        \n        print(f'Classification Report for {model}:', classification_result)\n        print(f'{model} Accuracy:', accuracy)\n                \n        # Print and write metrics to output file\n    output_file = 'StatsClassificationModelsGender3.csv'  # Replace with your desired output file path\n    with open(output_file, 'a') as w:\n        w.write(f'{model}, Accuracy: {accuracy:.4f}\\n')",
      "metadata": {
        "trusted": true,
        "tags": [],
        "editable": true,
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Classification Report for DecisionTree:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.81      1.00      0.90        30\n           1       0.56      0.93      0.70        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.65       103\n   macro avg       0.34      0.48      0.40       103\nweighted avg       0.45      0.65      0.53       103\n\nDecisionTree Accuracy: 0.6504854368932039\nClassification Report for KNeighbors:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.88      1.00      0.94        30\n           1       0.58      1.00      0.73        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.68       103\n   macro avg       0.37      0.50      0.42       103\nweighted avg       0.48      0.68      0.56       103\n\nKNeighbors Accuracy: 0.6796116504854369\nClassification Report for SVR:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.72      0.97      0.83        30\n           1       0.52      0.82      0.64        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.60       103\n   macro avg       0.31      0.45      0.37       103\nweighted avg       0.41      0.60      0.49       103\n\nSVR Accuracy: 0.6019417475728155\nClassification Report for NeuralNetwork:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.74      0.87      0.80        30\n           1       0.51      0.88      0.65        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.59       103\n   macro avg       0.31      0.44      0.36       103\nweighted avg       0.42      0.59      0.48       103\n\nNeuralNetwork Accuracy: 0.5922330097087378\nClassification Report for LogisticRegression:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.81      1.00      0.90        30\n           1       0.56      0.93      0.70        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.65       103\n   macro avg       0.34      0.48      0.40       103\nweighted avg       0.45      0.65      0.53       103\n\nLogisticRegression Accuracy: 0.6504854368932039\nClassification Report for RidgeClassifier:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.59      1.00      0.74        30\n           1       0.44      0.57      0.50        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.51       103\n   macro avg       0.26      0.39      0.31       103\nweighted avg       0.34      0.51      0.41       103\n\nRidgeClassifier Accuracy: 0.5145631067961165\nClassification Report for Voting:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.81      1.00      0.90        30\n           1       0.56      0.93      0.70        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.65       103\n   macro avg       0.34      0.48      0.40       103\nweighted avg       0.45      0.65      0.53       103\n\nVoting Accuracy: 0.6504854368932039\nClassification Report for RandomForest:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.83      1.00      0.91        30\n           1       0.57      0.95      0.71        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.66       103\n   macro avg       0.35      0.49      0.40       103\nweighted avg       0.46      0.66      0.54       103\n\nRandomForest Accuracy: 0.6601941747572816\nClassification Report for GradientBoosting:               precision    recall  f1-score   support\n\n          -1       0.00      0.00      0.00         4\n           0       0.83      0.97      0.89        30\n           1       0.56      0.95      0.70        40\n           2       0.00      0.00      0.00        29\n\n    accuracy                           0.65       103\n   macro avg       0.35      0.48      0.40       103\nweighted avg       0.46      0.65      0.53       103\n\nGradientBoosting Accuracy: 0.6504854368932039\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": "import glob\nfile_pattern = 'StatsClassificationModelsGender*.csv'\noutput_file = 'StatsClassificationModelsGender_combined.csv'\n\nwith open(output_file, 'w') as output:\n    for filename in glob.glob(file_pattern):\n        with open(filename, 'r') as input_file:\n            output.write(input_file.read())",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}